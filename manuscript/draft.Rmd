# Title

Authors: Harris, Taylor and White?

## Introduction

Quality of predictions is important:
  * helps determine policy/set priorities
  * something something climate change
  * need to know both accuracy of point estimates and how reasonable our prediction intervals are.
  * Overconfident predictions can be worse than uncertainty
  * Quality of prediction is one metric for how much we know about ecological processes
Unclear how much to trust existing predictions:
  * Strong assumptions (see "misc" below)
  * many *interesting* predictions
  * Potential problems with confidence intervals
  * Potential disconnect between training target and actual goals
  * Difficult to test model assumptions/predictions because of time scales
    * Most scientists making predictions today will have retired by 2050
Demonstrate best practices \& their contribution to better predictions
  * Use long-term data sets such as BBS to evaluate predictive skill
  * Note the large change in average temperatures since the 80's; comparable to the changes expected in the next N years (change since 80's is a true trend but whether it continues in uncertain, 10.1007/978-3-319-03768-4_2, 10.1007/s00382-014-2070-3)
, 
  * Lots of talk about how we should do it, fewer examples of doing them all together
  * Examples of best practices
    * not just bioclim
    * time series
    * spatial/site-level stuff
    * probabilistic/error bars
    * multiple models
    * Test with simple baselines (e.g. random walk, future-is-like-present)
  * Focus on richness/diversity because \[citations saying that it's important\]
  * Publish forecasts at different timescales to evaluate performance/credibility
Here we do that:
  * forecasts for biodiversity in NAmerican birds with as many best practices as possible
  * Discuss the influence of these improved methods \& of our error rates on our ability to understand:
    * How to improve
    * How much to trust the predictions

## Methods

**Data**

* BBS + retriever
* Monthly climate data from interpolated weather stations (PRISM)
* NDVI (GIMMS)
* Altitude (SRTM 90m Digital Elevation Database via raster::getData)

**Models**

* Mixed model for observer effort \& site effects
* Naive AR1 model (adjusted for observer effects)
* species-level GBM w/ mixed model point estimates as bonus predictors where available
* richness-level GBM w/ mixed model point estimates as bonus predictors where available
* JSDM
* Ensembles (weighted and unweighted)

(need a side note about why we're not using a proper count distribution; underdispersion, easier access to out-of-the-box time series methods)

**Evaluation**

* (R)MSE
* Log-likelihood (simultaneously rewards good point estimates, precision, and coverage)
* Coverage at 95%
* Calibration/bias
* Evaluated each year after the last training year
* Dig around in the residuals for insights

## Results and conclusions

* None of the models actually work all that well, compared to baseline.
* Temporal richness variability doesn't track the predictors we use; sites have "inertia"
  * Cite 3 papers Ethan found on consistancy of alpha even when composition changes dramatically
  * Good for absolute accuracy, little room for improvement over baseline
  * Maybe richness is too coarse
    * abundance
    * species-level
* More important to use a reasonable error model than to get the best predictors \& basis expansions
* Ensembles help/don't help (depending on results)
* Point estimate error versus coverage
* In many ways, this is the best-case scenario for SDMs:
  * large data set, known \& consistent sampling technique over a period of decades
  * "True" absences
  * extended predictor set
  * birds arguably migrate quickly enough to make equlibrium \& especially space-for-time assumptions less ridiculous
  * errors for individual species can cancel out; only asking for aggregate values
* Value of stacking versus predicting the quantitity of interest directly
* Need to start doing this, even if we're bad at it.
  * We've posted predictions for the next N years of BBS data, using several methods, and invite others to do so as well.


## misc

SDM assumptions:
  * we've measured the predictive power of all avialable variables, other potentially important thing that aren't available are landcover, observer skill level, (probably some others)
  * Something like stationarity:
    * e.g. "systems are at equilibrium now and will be at equilibrium in future"
    * or "species track predictor variables quickly" (no lags)
    * etc
  * independent/identically distributed observations across species, years
  * Space substitutes well for time

None of the models are true forcasts as they use observed predictor variables (except the AR1). 
